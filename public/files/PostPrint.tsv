Full Text	GPT-4 Summary	GPT-4 Title	
Introduction to Postprint			
Musing, you hold two books, one in your left hand, the other in your right. The left-hand book is a first edition of William Faulkner’s classic novel The Sound and the Fury (1929), for which you paid a staggeringly extravagant sum at a rare-book dealer. The right-hand book is a copy of the same novel that you bought on Amazon for a tiny fraction of that cost. Although you may not know it, the right-hand book was printed by CreateSpace, a digital publishing platform, at a nearby location to ensure you would receive it the next day. The bindings of course are different, but other than the colophon and publication date, all the words in both texts are the same. What do the differences matter, you think, except to a bibliophile like you, who prizes the aura of a first edition? Both are print books, aren’t they?	Two copies of the same novel are compared: a costly first edition and a cheap, print-on-demand version, raising the question of their differing value to a book lover.	Perceived Value in Books	
The contradictory perceptions articulated here express the conundrum at the heart of contemporary print books. On the one hand, as Alexander Starre puts it, “the printed book as it is sold in bookstores or shelved in university libraries is essentially the same as it has been since Gutenberg invented moveable printing.” As Starre’s careful qualification of shelved books hints, this illusion can be maintained only if the print book is regarded as an object in isolation. As soon as contexts are brought into view, the picture changes dramatically, represented in my example by noting the “aura” of the pricey first edition versus the print-on-demand knock-off. These contexts reveal that every aspect of print has been utterly transformed from Gutenberg’s (or Virginia Woolf’s) time, including how print books are composed, edited, designed, manufactured, warehoused, distributed, inventoried, sold, and read. This book argues that it is time—indeed, past time—to create a vocabulary and conceptual framework acknowledging the sea change that occurred when computational media permeated the printing industry. Like the slow accretion of chemicals on objects submerged in ocean water, this change took a half century or more to be fully realized, beginning roughly around 1950 and reaching full penetration with the new millennium. The term I propose to designate this new state of affairs is postprint.	Print books have evolved considerably since Gutenberg's time due to computational media's influence, altering every aspect from production to consumption. This shift, termed "postprint," demands a new vocabulary and understanding.	Unveiling the  Postprint Revolution	
“Oh no,” I hear you groan, “not another ‘post-’!” Yes, I know, it’s a drag, but it honestly seems the most appropriate term to describe where we are now with respect to the contemporary production and consumption of bound books. Post-, as always, implies both succession and displacement, continuation and rupture. For clarity, I use print to refer to the long reign of bound books produced by printing presses from the fifteenth century to the mid–twentieth century and postprint for books produced after roughly 2000. The half century of innovation between 1950 and 2000 marks the decades when computational technologies transformed the printing industry. Of course, many innovations preceded this period, including lithography (1796), chromolithography (1837), rotary presses (1843), and offset printing (1875). But these changes were incremental, affecting only a small portion of the print regime. The period from 1950 to 2000 represents a difference in scale so dramatic that it amounts to a qualitative, not merely quantitative transformation. It is no exaggeration to say it constitutes a rupture in the genealogy of printed books—the rupture denoted by the term postprint. To find a transformation of comparable import, we would need to reference the invention of moveable type and the Gutenberg printing press. Although numerous studies have explored the implications of the printing press and the massive economic, social, cultural, and religious changes it brought about (including studies by Elizabeth Eisenstein and Adrian Johns, among others), the full story of postprint is only beginning to be written. This book aspires to contribute to that effort.	The term "postprint" refers to the era after 2000, marked by a significant, qualitative transformation in book production due to computational technologies. This change, comparable to Gutenberg's movable type invention, is just beginning to be fully understood and studied.	Defining the  Postprint Era	
What elements make postprint distinctively different from print? One approach would be to reference the differences that code makes in the material production of texts. The book artist and theorist Amaranth Borsuk captures this aspect of the computational transformation when she notes that “a text’s digital life untethers it from any specific material support, making it accessible through a variety of interfaces (including the computer, cell phone, tablet, and dedicated e-reader), each of which influences our reading.” The codes underlying digital texts position print in a profoundly different way than it was positioned in the previous epochs, for in the postprint era hard copy becomes merely one kind of output among many possible displays. As Borsuk concisely observes, “When books become content to be marketed and sold this way [as e-books], the historic relationship between materiality and text is severed.” I would modify her observation by adding that although the specificity of display varies, as she notes, it is important to realize that digital forms are also instantiated in specific platforms, operating systems, coding protocols, and display mechanisms. The difference is emphatically not between the materiality of print and the immateriality of digital forms, as is sometimes proclaimed, but rather between different kinds of material instantiations and diverse kinds of textual bodies.	Postprint is distinguished from traditional print by its integration of code in text production, allowing text to be untethered from specific material supports and accessed via various digital interfaces. In this era, hard copy is one of many possible displays. However, digital forms also rely on specific platforms, systems, and codes, making the key difference not about materiality versus immateriality, but rather about differing material instantiations and diverse textual bodies.	Materiality and Digital Transformation	
To enhance the reader’s awareness of the code layers essential for generating the print page in the postprint era, this book features ten black pages with white ink that display one or more of the code layers necessary for producing the facing white page. By analogy with X-rays that reveal the body’s otherwise visually inaccessible skeletal frame, these pages may be considered “X-ray” versions of the print page, revealing the code architectures hidden from the print reader’s view but essential for the (post)print reading experience. Captions identify the relation of the displayed code to the print page, and readers are invited to read across the page spread, correlating black with white, code with output.	The book includes "X-ray" pages showing the hidden code behind the print page, emphasizing the integral role of this code in the postprint reading experience.	Revealing Code Layers in Postprint	
Moreover, this design alludes to the phenomenon that Jessica Pressman calls the “aesthetic of bookishness.” She argues that the advent of the digital has had the paradoxical effect of stimulating widespread interest in the history of the book, with a flowering of scholarship in this area that is truly remarkable. Our collaboratively coedited essay collection Comparative Textual Media: Transforming the Humanities in the Postprint Era, which was among the first to put the term postprint into circulation, brought together some of this research to demonstrate that digitality has not endangered print books, as Jeff Gomez asserts in Print Is Dead, but rather has opened possibilities to compare different forms of textual media by engaging in what I have elsewhere called “media-specific analysis.” Here media-specific analysis takes the form of visual presentations of code along with print pages, suggesting that traditional forms of reading print may now be augmented to include reading the underlying code as well. The idea of media-specific analysis grew out of my long interest in the materiality of texts, shown perhaps most extensively in my artist’s book collaboration with designer Anne Burdick, Writing Machines. Almost a decade ago, then, I was already experimenting with the idea of “postprint,” but it has taken the intervening years for me to have the time and framework through which I could make this more than a rhetorical assertion. Now, with this book, I present a fully fleshed-out argument based on archival research, interviews, and, perhaps most importantly, an encompassing conceptual framework within which to position postprint as part of larger social, economic, and cognitive transformations. I see these transformations as analogous to other world-changing and species-defining cognitive developments such as the inventions of language and literacy. Borsuk makes much the same point when she succinctly observes, “It bears emphasizing that writing itself fundamentally changed human consciousness, much as our reliance on networked and digital devices has altered us at the core."	The digital era, paradoxically, has renewed interest in book history. The term "postprint" is used to encapsulate this shift where traditional print reading is now complemented with reading the underlying code. This book presents a comprehensive argument for postprint, considering it as part of broader social, economic, and cognitive changes, akin to transformative developments like the invention of language and literacy. These alterations fundamentally change human consciousness, similar to the effects of reliance on digital and networked devices.	Augmenting Traditional Reading	
 Why Cognition? 			
How to capture the essence of these transformations? Although the interjection of code into textuality captures limited aspects of the change, the transformation’s full scope is much broader. My approach has been to scale up (and down) to what I see as the catalyst for all of postprint’s diverse aspects—namely, the emergence of cognition in artificial media. Just as the advent of the digital has catalyzed the history of the book by bringing the codex back into visibility, shattering its taken-for-granted quality acquired through centuries of habituation, so the emergence of cognition in artificial media has stimulated a reconceptualization of cognition in general, including biological cognition. Through several books, including How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and Informatics; My Mother Was a Computer: Digital Subjects and Literary Texts; How We Think: Digital Media and Contemporary Technogenesis; and especially Unthought: The Power of the Cognitive Nonconscious,9 I have explored the relationship of human cognition to computational media and to cognition generally. This journey has also involved reexamining the role of cognition in human life, for which I drew on recent work in neuroscience, cognitive science, and related fields demonstrating that cognition is much broader and deeper than consciousness. To make the argument effectively, I obviously needed to define cognition. To emphasize that cognition includes consciousness but is not limited to it, I wanted to position it so it would include affective responses, internal and external sensory systems, nonconscious cognition, and other embodied, enactive, and embedded bodily capacities. I also wanted it to connect with contexts that include internal and external milieu. After considerable deliberation, here is what I came up with: “Cognition is a process of interpreting information in contexts that connect it with meaning.” This definition has the advantage of conciseness; it also invokes two terms especially potent within the humanities, interpretation and meaning, which also require more explication.	This transformation extends beyond code interjection into textuality; it encapsulates the emergence of cognition in artificial media. As digital advent revived the history of the book, cognition's emergence in artificial media prompted a reconsideration of cognition, including biological cognition. This includes an expansive understanding of cognition to encompass consciousness, affective responses, sensory systems, nonconscious cognition, and other bodily capacities. Therefore, cognition is defined as the process of interpreting information in contexts that connect it with meaning. This brings potent concepts from humanities into focus - interpretation and meaning.	Non-human cognition: biological and artificial interpretation	
Starting from this foundation, I argued that all biological life-forms have some cognitive capacities, even those without brains, such as nematode worms and plants. To reposition the terms meaning and interpretation, I drew from biosemiotics, a field that studies sign creation, interpretation, and dissemination among nonhuman species. For biosemioticians such as Jesper Hoffmeyer, Terrence Deacon, and Wendy Wheeler, meaning-making practices are essentially behaviors evoked by environmental changes, such as a deciduous tree losing its leaves in winter.10 In this minimal sense, all life-forms perform interpretations on information coming from their environments (for multicellular organisms, also coming from their internal sensory systems), which are meaningful to them within their world horizons (or Umwelten, as Jacob von Uexküll puts it).	Cognitive capacities extend to all biological life-forms, including those without brains, such as nematode worms and plants. Meaning and interpretation are redefined through biosemiotics, studying sign creation and interpretation among nonhuman species. Environmental changes incite behaviors that create meaning, and life-forms interpret environmental information within their own context.	Cognition Across Life-Forms	
This framework radically shifts the position of humans in relation to other life-forms. Rather than emphasizing human uniqueness (along with the inevitable corresponding denigration of other species), it emphasizes continuities of cognition across the biological spectrum while still acknowledging the specificities of each species’ capacities. At the same time, it also recognizes humans’ special ability to take responsibility for those attributes associated with symbolic reasoning and higher consciousness, such as the ability to formulate complex plans, implement them, and recognize large-scale patterns and significances. Taking responsibility positions humans not as dominators of the planet but as caretakers of it for their own benefit, for other species, and for the planet-wide systems of material processes on which all life depends.	The framework aligns humans cognitively with other life-forms, highlighting continuities rather than uniqueness. However, it acknowledges human capabilities for complex reasoning and responsibility, positioning us as planet caretakers, beneficial for all life and the Earth's systems.	Repositioning Humans in the Web of Life	
In addition to recontextualizing human cognition in relation to other life-forms, this approach also enables a different understanding of computational media relative to human cognition. Although many biosemioticians argue that computers cannot create meanings because they are not autonomous, this objection is an arbitrary limitation belied by what computers actually do. As far I know, no computational media are conscious. Nevertheless, like nonconscious organisms, computers have internal and external milieus: they process and interpret information, including information from sensors and actuators when these things are present, and they create meanings in the sense of performing behaviors that have efficacy within their environments. Because cognition is inherently embodied, making these claims does not obscure or avoid the fact that computational media have profoundly different instantiations—that is, bodies—than do biological organisms, nor does it imply that computers operate like brains or vice versa.	The approach redefines our understanding of computational media, arguing that computers, akin to nonconscious organisms, interpret information and create meaningful behaviors within their environments, despite their unique physical forms.	Computational Media and Meaning	
Thus repositioned, cognition spans all three domains—humans, nonhumans, and computational media—while foregrounding the importance of specific embodiments and embeddings across the domains. Moreover, the extension of human cognition through computational media has been and in all likelihood will continue to be a major driver in human evolution now and in the foreseeable future. Human cognition no longer relies solely on the glacial pace of biological evolution but now proceeds through the exponentially faster processes of cyber-bio-evolution, where evolution through generational change is measured in months rather than in hundreds of thousands of years. Wherever cognitive processes take place—in humans, nonhumans, and technical devices—they bestow the advantages of flexibility, adaptability, and evolvability. At the same time, all cognitive processes depend on material processes, such as chemical reactions, that are not cognitive in themselves because they do not perform interpretations, make choices or selections, or create meanings. It took the spark of life to activate those possibilities, and it took millions of years more for that spark to evolve into life-forms capable of creating similar cognitive possibilities in artificial media.	Cognition, spanning humans, nonhumans, and computational media, underscores the significance of unique embodiments across these domains. The human cognitive evolution has shifted towards cyber-bio-evolution due to computational media, bringing quicker adaptability and evolvability. Despite this, all cognitive processes remain reliant on non-cognitive material processes like chemical reactions. The evolution of life has activated these possibilities, eventually leading to artificial media with cognitive potential.	Cognition Across Domains: Cyber-Bio-Evolution	
Although my definition of cognition focuses primarily on individual cognizers, I also want to emphasize that interpretations and meaning-making practices circulate through transindividual collectivities created by fluctuating and dynamic interconnections between humans and computational media, interconnections that I call cognitive assemblages. Much of the world’s work in developed societies is increasingly done through cognitive assemblages. You may not think about it in these terms, but when you activate your car’s starter motor by means of a key with an embedded chip, you are participating in a cognitive assemblage. That sequence can be described as follows: the embedded key chip links up with your vehicle’s computer and allows the ignition button to activate and complete an electrical circuit, initiating a chain of events in which a current is fed to the solenoid, which turns on an electromagnet, attracting an iron rod, whose movement closes two heavy contacts, thus completing the circuit from the battery to the starter, which then turns the shaft so that the pinion (or gear wheel) engages with a large gear ring around the rim of the engine flywheel, which begins to turn at increasing speeds, and voilà!—your car starts. This sequence used to be purely electromechanical, but now virtually all contemporary cars have keys with embedded chips and vehicle computers, which typically work to control, disseminate, and modulate information. Similarly, you are participating in cognitive assemblages when you talk on your cell phone, turn on a light switch, fly on a plane, take the subway, order groceries on Amazon, use your credit card to pay a restaurant bill—each of these everyday activities is interfaced with computational media such that it will not work without them. As different entities, human and nonhuman, enter into communication and information circuits, the assemblage fluctuates in its groupings and functions. The cognitive-assemblage framework aims to be flexible enough to accommodate these differences while still being specific enough to be useful.	Cognitive assemblages, dynamic interconnections between humans and computational media, facilitate the circulation of interpretations and meaning-making practices across collective entities. Many everyday actions, from starting a car to online shopping, involve participation in these assemblages, interfacing with computational media. As various entities communicate and exchange information, the assemblage's functions and groupings fluctuate. The cognitive-assemblage framework is designed to handle these differences while maintaining its practicality.	Cognitive Assemblages in Human-Computational Systems	
If we ask why computational media deserve primary emphasis in the framework developed here, the answer may not be immediately obvious. They are not, for example, the most impactful for ordinary human lives; that honor might go to transportation networks, from dirt paths to jet aircraft. Nor are they the most important for human well-being; sanitation facilities and water-treatment plants would be better candidates for that nomination. But transportation networks, sewage-disposal systems, and water-treatment plants are nowadays likely to have computational components as controllers, communicators, distributors, and interpreters of information flows. As with an automobile starter circuit, these computational components intervene in previously developed technologies to make them more flexible and adaptive to changing circumstances. It is precisely their cognitive capabilities—their ability to interpret information flows in ways that connect them to contexts of meaning—that account for their deep penetration into virtually every aspect of contemporary life and for the enormous effects they are having on how we humans live our lives and the kind of creatures we understand ourselves to be.	Computational media, due to their ability to interpret and adapt information, are integral to modern life and shape our self-perception, despite less direct impact compared to other systems like transportation or sanitation.	The Cognitive Significance of Computational Media	
In (very) broad outline, then, the situation looks like this: It took a few million years for biological evolution to result in Homo sapiens, the first species to engage extensively in abstract thought and symbolic reasoning. Humans are the cognitive species par excellence, and we have used these abilities to instantiate symbolic reasoning and logic in artificial media, which now have their own evolutionary dynamics. In suggesting a parallel between biological and artificial evolution, I do not intend to minimize the significant differences between them. These differences can be understood through what I call the two great inversions. The first inversion replaced the biological mandate “survive and reproduce,” the process through which biological evolution proceeds, with the computational mandate “design and purpose.” Computational systems, unlike biological organisms, are designed for specific purposes, so their evolution in this sense proceeds in top-down fashion.	 Over millions of years, biological evolution led to humans, unique in their abstract thinking and symbolic reasoning. Humans have transferred these abilities into artificial media, initiating a novel evolutionary pathway. This 'artificial evolution' signifies a shift from the biological "survive and reproduce" mandate to a design-focused, purpose-driven computational directive.	The Interplay of Biological and Artificial Evolution	
The second great inversion concerns the directionality of evolutionary trajectories. From day one, biological organisms had to be able to cope with fluctuating and unpredictable environments; if they could not, they did not survive. From this foundation, a cognitive trajectory toward increasing complexity eventually led to the capability for abstract thought. Computational media, by contrast, instantiate abstract thought at a foundational level in the logic gates, which are almost completely deterministic. From this base, layers of software increasingly enable computational media to deal with uncertain and ambiguous information. When sensors and actuators are added, computational systems may also be able to deal with environmental ambiguities and fluctuations. In addition, developments beyond von Neumann architectures, such as neural nets and neuromorphic chips, further advance in the direction of fault-tolerant systems able to draw inferences from unruly data and to operate successfully even with contradictory information.13 In this sense, then, the trajectory of computational evolution proceeds in the opposite direction from biological evolution. The biological progression is from uncertainties up to abstraction, which the computational progression inverts so that it proceeds from abstraction up to uncertainties.	Biological organisms had to adapt to fluctuating environments to survive, eventually leading to abstract thinking. Computational media, however, started with abstract logic gates that are highly deterministic. Over time, software layers, sensors, actuators, and advanced architectures have enhanced their ability to manage uncertain and ambiguous data. Thus, biological evolution moves from uncertainty to abstraction, while computational evolution moves from abstraction to uncertainty.	The Inverted Trajectories of Biological and Computational Evolution	
In summary, my focus on cognition is intended to capture the aspects of computational media that make them not just another technology but a cognitive technology able to interpret, disseminate, and contextualize flows of information so they become meaningful within specific contexts. This is why these media are able to impart to a huge variety of other technologies the advantages that cognition bestows—flexibility, adaptability, and evolvability. Once mechanical processes are interpenetrated by computational components able to interpret information and create meaning from it, they are able to carry out tasks impossible for machines without cognitive abilities.	Cognition is key to computational media, making them cognitive technologies that interpret, distribute, and contextualize information. This cognition grants flexibility, adaptability, and evolvability to other technologies, enabling tasks impossible for non-cognitive machines.	Cognitive Technology: Beyond Mechanical Processes	
For example, mobile robots stacking boxes in a warehouse are able to do tasks that would be impossible for a forklift lacking a human driver.14 The forklift by itself cannot navigate around obstacles; it is unable to judge properly how to lift and stack boxes, nor does it know when to start and stop. The forklift can operate only when human cognition comes to the rescue and provides the cognitive abilities that it lacks. But as the box-stacking robots demonstrate, incorporating computational components into machines now enables machines to take on tasks that formerly could be done only by humans.	Incorporating computational components lets machines handle tasks previously only possible by humans, like mobile robots stacking boxes autonomously.	Autonomous Machines Surpassing Human Tasks	
Of course, humans create, program, implement, and maintain these cognitive devices (by operating within the relevant cognitive assemblages). Nevertheless, the intervals in which cognitive technologies operate autonomously are becoming both more numerous and longer in duration. Self-driving cars, already in use, are an obvious example, but they are the tip of the iceberg. Much of the infrastructure of developed societies already incorporates and completely depends on cognitive technologies, including electrical grids, communication networks, currency and monetary exchanges, transportation vehicles in all their forms, and so forth. In fact, I will make the bold claim that daily life in developed societies can proceed in normal fashion only because of the incorporation of cognitive assemblages, without which crucial infrastructural systems simply would not function.	Cognitive technologies are increasingly operating autonomously, extending beyond self-driving cars to numerous societal infrastructures like electrical grids, communication networks, and transportation systems. It's a bold but accurate claim that developed societies depend on these cognitive assemblages for their daily operations.	Rise of Autonomous Cognitive Technologies	
 Cognitive Assemblages and Becoming Computational 			
As a phrase, cognitive assemblage, like Janus, faces in two directions at once. One face turns toward the embodied, embedded, and enactive processes of cognition, which are always species specific, related to specific contexts and circumstances, circumscribed by the sensory and cognitive capacities of organisms that create distinctive world horizons, and performed by individual entities. The other turns toward fluctuating collectivities through which information, interpretations, and meanings circulate. It foregrounds connections between entities, circuits of stores and flows, resistances and affordances that they inflect and direct, and the diverse meanings partially shared and partially obscured that emerge through those connections and circuits. Although each of the connections can be specified through the materialities creating them, the flow allows information to travel among different substrates and diverse interpretation media. In this sense, the cognitive-assemblage framework enables two different kinds of discourses: one that focuses on the materialities of individual participants and another that focuses on the more abstract flows that bind entities together into a collectivity.	 "Cognitive assemblage" refers to both the individual cognitive processes, which are unique to specific organisms, and the collective flow of information, interpretations, and meanings among these entities. This framework acknowledges the material aspects of individual entities, as well as the abstract flow of information that connects these entities into a shared network.	Dual Perspectives of Cognitive Assemblages	
It is worth emphasizing that a cognitive assemblage may include conscious participants, but its functionality does not require it to do so. It can also operate through nonconscious cognition and with nonhuman entities as well as with humans. A cell phone call, for example, means one thing to the person receiving it, another thing to the phone receiving/producing it, and still something else to the repeating tower used to help transmit it. If the human is cut out of the loop, for example, when a user does not answer and the call goes to voicemail, then all of the participants receiving and interpreting the call are nonhuman computational cognitive entities.	A cognitive assemblage can function with nonconscious, nonhuman participants. For instance, in a phone call, various interpretations occur by the receiver, phone, and tower, all functioning even if the call goes to voicemail.	Cognitive Assemblages Without Human Involvement	
In appropriating the term assemblage, I draw on the work of both Bruno Latour and Gilles Deleuze and Félix Guattari, with some differences. In Latour’s invocation of assemblage, often used as a synonym for network, objects with no cognitive capacities, such as a hammer or pulley, are agents just as much as cognitive entities are (although, in fact, Latour rarely instances cognitive technologies such as computers, preferring to remain in the realm of mechanical devices).15 In contrast, my view of assemblage makes a clear distinction between cognitive entities and noncognitive ones. In Deleuze and Guattari’s texts, “assemblage” is often how the term agencement is translated. The French term better captures the dynamism they envision for an assemblage, in which components are always on their way to becoming something else. In contrast to their usage, I assume that entities may be relatively stable and preexist the assemblage in which they participate. Another inspiration for me is Deleuze and Guattari’s notion of “becoming” (as in “becoming-animal” or “becoming-woman”), which for them is a result of seeing entities as more or less temporary arrangements, with special emphasis on movements from majoritarian configurations to minoritarian ones.16 Of course, Deleuze was highly influenced by the work of Gilbert Simondon, another powerful thinker about the way transindividuation (movements beyond the individual that form a consistent aggregation) takes place, as well as by Alfred North Whitehead, whose processural philosophy also heads in this direction.17 My hesitation about adopting such philosophical positions wholesale has to do with what I see in Deleuze and Guattari as their neglect—indeed, almost complete erasure—of the mechanisms that work toward preserving the stability of entities, from the skin that for mammals is the body’s largest organ to the fundamental role of DNA in conserving information across the generations of biological life-forms to the systemic dynamics that make computational media distinct as well as interconnected. Such stabilizing mechanisms are always in dynamic tension with destabilizing ones, but to emphasize only destabilization is as distorting as to emphasize only stabilization (for example, the mistaken focus on DNA as the sole arbiter of how inheritance works even though it is now recognized to be in dynamic interaction with epigenetics and other mechanisms). These reservations are expressed typographically in my subtitle by the absence of the Deleuzian hyphen in “becoming computational.”	The term "assemblage" is used with influences from Bruno Latour, Gilles Deleuze, and Félix Guattari, though with differences. While Latour includes non-cognitive objects as agents in his assemblages, this perspective distinguishes between cognitive and noncognitive entities. The Deleuzian-Guattarian concept of assemblage, capturing dynamic component transitions, is also drawn upon, but with allowance for relatively stable preexisting entities. The idea of "becoming", from Deleuze and Guattari, informs this view, but there's a resistance to ignore the stabilizing mechanisms of entities. The author sees a necessary balance between stabilization and destabilization dynamics in entities.	Interpreting 'Assemblage': Stability and Transition	
The idea that the evolution of computational media is now completely entwined with human evolution strikes me as a conclusion as inescapable as it is potent with implications for the future of humans and the planet. In my work in progress, “Cyber-bio-evolution: A General Ecology of Cognitive Assemblages,” I explore this idea in depth, which I call cybersymbiosis. We (humans) are becoming computational in one way or another, just as computational systems are edging toward the biologically fundamental processes of dealing with uncertainties and ambiguities. If we are to believe the intimations of our science fiction writers, perhaps computational systems are also evolving toward the biological mandate of survival and reproduction (as in films such as Terminator, Ex Machina, etc.). It follows that books in all their diversity, as facilitators of and interactors with human cognitions, are also in the process of becoming computational. My focus is not only books by themselves, however, but also books in the context of encompassing cognitive transformations. The conjunctive and in my subtitle is meant to position their becoming computational as one aspect of a much larger picture: the becoming computational of humans and, indeed, of the entire planet.	The evolution of computational media is inextricably linked with human evolution, a phenomenon termed as "cybersymbiosis". Both humans and computational systems are increasingly adapting to each other, particularly in handling uncertainties. This integration impacts diverse mediums, notably books, which are transforming to facilitate human cognition in this computational environment. The transformation of books is just a fragment of a broader paradigm shift involving humans and the planet at large becoming more computational.	Cybersymbiosis: The Shared Evolution of Humans and Technology	
 Cognitive Assemblages and Postprint 			
What kind of work can the cognitive-assemblage framework do in relation to postprint? It provides a way to conceptualize innovations in print technology as redistributions of cognitive capabilities between humans and machines. Such an approach enables new questions to be asked about the benefits and limitations of the emerging assemblages—a crucial point, for example, in chapter 2 in explaining why the Paige Compositor was rendered obsolete by the Mergenthaler Linotype typesetting machine. It also provides a way for transformations in printing machines to be linked to corresponding changes in editing, designing, producing, selling, and consuming books because at a foundational level all of these changes have resulted from the introduction of computational media. Many branches grow from this trunk, and although they may appear different in function, form, and placement along the production cycle, tracing them back to the root transformation enables us to understand them as related effects and not simply as different kinds of phenomena. Postprint reveals this convergence by locating print within the computational-media ecology in which it currently exists.	The cognitive-assemblage framework offers a perspective to view print technology innovations as redistribution of cognitive capabilities between humans and machines, illuminating transformations across all aspects of book production due to the introduction of computational media, thereby situating print within the current computational-media ecology.	Cognitive Assemblages in Postprint Innovations	
In addition, postprint suggests new kinds of connections with existing fields of inquiry. The area centrally concerned with print technologies in relation to books is of course the history of the book. The recent flourishing of scholarship in this area has made many important contributions, especially in its attention to the materiality of texts and the everyday practices of literary production and consumption. Nevertheless, the kind of wide-ranging yet coherent approach I advocate here is still underdeveloped and underappreciated within the discipline as a whole, perhaps because of a lingering distrust of and ignorance about computational technologies and their relation to literary enterprises. Only in the past few years have studies emerged that link the history of the book with computational media, such as Johanna Drucker’s SpecLab: Digital Aesthetics and Projects in Speculative Computing; Matthew Kirschenbaum’s meticulously researched Track Changes: A Literary History of Word Processing; Dennis Tenen’s Plain Text: The Poetics of Computation; and Amaranth Borsuk’s The Book, among others.19 To my knowledge, my focus on cognition here pursues the link between the history of print and computational technologies along lines not previously explored. It brings to light new archival resources in the history of technology not generally acknowledged within studies of the history of the book; through interviews, it also develops connections with scholarly publishing practices that illuminate the complex ways in which humans respond when they find themselves having to deal with technologies they did not invent and may not completely understand but that they also cannot avoid.	Postprint builds upon book history, bringing new insights to the relationship between books and computational media. Despite some initial resistance, studies are increasingly linking these two fields, exploring the role of cognition in these connections. This approach unveils new archival resources and presents human reactions to new, yet unavoidable, technologies.	Bridging Print History and Computational Media	
The focus on cognition also serves as a springboard to further understand cultural phenomena: the ways that print has been enmeshed in what it means to be human in literate, developed societies and how these ways are changing as some of the cognitive tasks that once were performed exclusively by humans are now carried out by computational media. Such a change in the distribution of cognitive capabilities carries important and, indeed, momentous implications about the future of the human species as well as about the everyday practices of writing, editing, and producing books, a throughline that connects all the chapters.	Understanding cognition helps in studying the cultural significance of print in literate societies and how it's changing with the introduction of digital media. The shift from human to computer in performing cognitive tasks profoundly impacts the future of humanity and daily activities like writing, editing, and book production.	From Print to Digital Media's Impact on Society	
Finally, my exploration of this subject has a personal motivation as well. Like many academics in the humanities, I live a significant portion of my everyday life through and amid print books. They are my teachers, collaborators, companions, friends, and opponents urging me to up my game. Embarking on this book has thus for me been a way to ground theoretical ideas in an area crucial to my professional practice and central to my sense of self. In a word, I am deeply invested in print culture and in the transformations it is undergoing as it morphs into postprint. At the same time, this book also serves as a test bed to put on trial general ideas about cognition, evolution, cybersymbiosis, ethics in/as innovation, and human futurity to see which of them hold up and which don’t.	The investigation of print culture's shift is personally driven due to a profound connection with books. This research is a testing ground for theories on cognition, evolution, and human futurity.	A Personal Lens on Print Culture's Evolution	
 Postprint as Equivocal Figure 			
In a sense, the marriage I am proposing under the rubric of postprint is much stranger than that of a Montague and Capulet who have fallen madly in love because it joins not just two estranged lineages but multiple ones. Consider, for example, the relation between book studies and the history of technology. One would think the two should be closely related because the book, as book historians rightly delight in pointing out, is a result of complex technological processes. Yet they have much fewer crossovers than might be imagined, in part, I suspect, because of the aura and cultural capital of books compared, say, to a ball-bearing factory. Or think of the history of computing in relation to book studies. As mentioned earlier, only recently have studies appeared that link the two, even though, as I show in chapter 2, their fates have been entwined since the 1950s. Or, again, consider the cognitive studies of literature, an emerging field with several landmark studies and collections, and of electronic literature, the area of literary creation, discussion, and criticism that most directly connects to computation. With few exceptions, the two fields proceed along divergent tracks and have almost completely different archives of reference, principally because “cognition” in cognitive studies is invariably taken to mean human cognition without extending the idea to computational media.	The concept of postprint involves a complex and unconventional union of diverse fields such as book studies, history of technology, history of computing, cognitive studies of literature, and electronic literature. Despite inherent connections, these fields often lack intersections due to various factors, including cultural perceptions and different reference archives. The exploration aims to link these disparate fields, highlighting the shared history and evolving relationship between human cognition and computational media.	Intersecting Disciplines	
Yet the seemingly unconnected threads sticking out from these multiple fields cross one another all the time, and it takes only a shift in perception to bring the linkages between them into view. I think here of equivocal figures, such as the vase that suddenly is perceived as the profile of a young woman. I hope that the idea of machine cognition and the related concept of cognitive assemblages will stimulate a shift in perception that brings a new configuration into view for my readers, the result of a marriage between multiple partners with several different lineages. In my experience with equivocal figures, once both possibilities are perceived, it is possible to switch from one to the other by focusing on a point of connection—say, a pedestal indentation that becomes a projecting chin. Perhaps the term postprint may function as the verbal cue, akin to the pedestal indentation, that allows folks to see how the new configuration emerges from yet interacts with older perceptions, creating a rich sense of dynamic interaction between them. My intent is thus not simply to negate older configurations that assume that only humans have cognition but also to enhance them by putting them in different contexts of meaning and interpretation, processes central to my framework of cognitive assemblages but also operating within readers in relation to the new picture I hope to bring into visibility.	The seemingly disparate fields constantly intersect, and a shift in perception can reveal these connections. The concepts of machine cognition and cognitive assemblages could catalyze this shift, bringing a new multidisciplinary view. The term 'postprint' might serve as a cue to help perceive how this new configuration coexists and interacts with older perspectives. This isn't aimed at negating traditional views that associate cognition solely with humans but seeks to enrich them by putting them in new contexts, aligning with the concept of cognitive assemblages.	Postprint Cognition and the Shift in Perception	
 Recent Book Scholarship in Relation to Postprint 			
One of the lineages contributing to this new configuration is the history of the book. In the profusion of recent book studies, three trends have special relevance to bringing the postprint configuration into view: the impulse to synthesize multiple aspects of the book into a single picture; an emerging emphasis on everyday practices within the print world; and the appearance of narrative strategies specifically catalyzed by the conjunction of digital technologies with the print codex. Together, these trends reveal how the present study overlaps with published scholarship yet also how it opens new possibilities for continuing research.	 The 'postprint' configuration stems from book history, particularly through the synthesis of book aspects, focus on everyday print practices, and exploration of narratives in the digital-print nexus. This contributes to existing scholarship and offers new research paths.	Book History's Role in the Postprint Era	
In the first category of enlarging the scope, Amaranth Borsuk’s The Book is exemplary. Its very existence in MIT’s Essential Knowledge series testifies to the resurgence of interest in the history of the book and the growing scholarly attention to the codex form and its predecessors. The large scope becomes apparent when Borsuk writes that her aim is to consider the book as object, content, idea, and interface. An especially attractive feature is the confident way she handles the advent of digital technologies and the possibilities they present for new kinds of experimentation. Confronting the claim that digital texts will result in the death of books, she brushes aside the looming anxiety booklovers might feel by causally remarking, “That question isn’t as interesting, though, as the question of how each of these technologies [from the invention of moveable type to e-books] has been, and will continue to be, part of the book’s development.” Her chapter on artists’ books makes good on this perspective by including many hybrid print–digital productions as well as works of electronic literature. Although attentive to technological innovations and forms, Borsuk nevertheless locates the moment when literature happens in a reader’s mind: “Books are always a negotiation, a performance, an event: even a Dickens novel remains inert until a reader opens it up, engaging its language and imaginative world.” She even concludes her study by reinforcing the message: “All books, I hope this volume suggests, arise in the moment of reception, in the hands, eyes, ears, and mind of the reader.”	Amaranth Borsuk's The Book expands the scope of book history. It covers the book as an object, content, idea, and interface, confidently addressing the impact of digital technologies on the future of books. Borsuk dismisses fears of the death of books due to digital texts, focusing instead on how technologies contribute to book development. She includes hybrid print-digital works and electronic literature while emphasizing that literature comes alive in the reader's mind.	Amaranth Borsuk's The Book and the Expanded Scope of Book History	
This is a comforting thought for those who want to see the history of the book as a continuing engagement between the artifact and the reader’s mind that stretches unbroken across the centuries. It is also typical of older formations in putting sole emphasis on human cognition. Yet Borsuk knows as well as anyone that this is not the complete story and that in the contemporary era cognitive machine operations often intercede between book and (human) mind. We need go no further than her own creative work to have this key insight confirmed. Her innovative book Between Page and Screen (2016), coauthored with Brad Bouse, presents as a print codex but contains no words, only optical codes that when held up to a digital optical reader project word poems that chronicle a difficult and challenging love affair between P and S (discussed at length in chapter 5). In the arrangement that Borsuk and Bouse have set up, machine interpretations and meaning-making practices are the necessary antecedents for a human reader to perceive any words at all. The result is a book that instantiates itself as a cognitive assemblage, a dynamically interacting coproduction between machine and human cognition. What crucial clue can bring this changed perception into view? I like to think it is “cognition”—the realization that the machine performs not just any operations but specifically cognitive ones. Once this realization has crystallized, it becomes necessary to modify Borsuk’s closing claim that literature is an event that happens in a reader’s mind to a more precise formulation that literature is an event happening (sometimes) in a cognitive assemblage.	While the book history traditionally emphasizes human cognition, modern realities include machine cognition. Borsuk's Between Page and Screen exemplifies this interaction between machine and human cognition, suggesting that literature can be an event in a cognitive assemblage.	Cognitive Assemblage in Modern Literature	The concept of literature has evolved from a solely human-centered event to a dynamic collaboration between human and machine cognition within a cognitive assemblage.
A different encompassing approach is pioneered by Garrett Stewart in Book Text Medium: Cross Sectional Reading for a Digital Age. He comments that “this is the first account to venture in a fully material sense … a sustained conjunction of book studies, textual studies, and media studies: or, better put, book history, textual analysis, and media theory.”24 Somewhat idiosyncratically, he understands the term medium to refer to language rather than to a particular delivery vehicle. This perspective is at once innovative and clever, for it defines medium in such a way that it largely insulates his analysis from having to deal much with technology, positioning it instead to play to his strengths in the areas of verbal ambiguities, language play, and the material properties of book objects. For my money, no one is better at this game than Garrett Stewart. He excels in excavating homophonic puns from linguistic structures as well as in understanding in deep terms how the material properties of the codex can be used to create meanings. He demonstrates this in his earlier publication Bookwork: Medium to Concept to Object to Art, a study devoted to what happens when a book is demediated or altered by an artist so that it literally cannot be read and functions instead as an artifactual object stimulating new realizations in the mind of a viewer.	Garrett Stewart's Book Text Medium: Cross Sectional Reading for a Digital Age merges book studies, textual analysis, and media theory, interpreting 'medium' as language instead of a delivery method. This focus allows Stewart to delve into aspects like language play and the physical properties of books, reducing emphasis on technology. His work, Bookwork: Medium to Concept to Object to Art, further demonstrates his approach to book studies.	Garrett Stewart's Approach to Medium in Book Studies	Garrett Stewart's work in 'Book Text Medium' redefines 'medium' to focus on language, not delivery systems, enabling nuanced analysis of verbal nuances and physical properties of books.
Yet even Stewart finds opportunities to draw connections between his typical interests and computer interpretations—for example, when in Book Text Medium he discusses Borsuk and Bouse’s Between Page and Screen. His response is worth quoting at length for the connections it makes: "Uploaded here is a whole new implementation of the “picture book” as “illuminated” text. A very long leap, then, and not just technological but aesthetic. Even political, as we’ll see—since ceding to the computer the right to do our “book reading” for us, even when only a trope for exaggerated interactivity, has consequences not soon bottled up. A transhistorical leap, to be sure, is needed to move from one end of this long cultural spectrum to the other, from sacred painting to such interactive computer graphics. But recent phases of the intervening terrain have been variously occupied—and rigorously thought (as well as passed) through—by several decades of resistant conceptualist invention in the work of words"	Stewart embraces computer interpretations in his analysis of 'Between Page and Screen', identifying it as a reinvention of the "picture book". This transition from traditional books to interactive digital text prompts significant shifts in aesthetics, technology, and even politics, highlighting the impact and evolution of word art in the digital age.	Leap from Books to Screens	The shift from traditional reading to computer-mediated interaction signifies a transformative leap across aesthetics, technology, and politics, showcasing the evolution of conceptualist invention in the world of words.
We see here the turn from direct consideration of the technology (having the computer do our reading for us) to more familiar territories of conceptual art and the “work of words,” but not before Stewart acknowledges that having a computer read initiates “consequences not soon bottled up.” Indeed! It would take only a gentle nudge, I think, for the new configuration I am limning here to appear from this vision of conceptual and verbal art as interventions in the way humans see ourselves and the objects we create. What’s missing is a sense that computers are also capable of cognition and that they, too, produce interpretations and meanings that circulate within machine contexts as well as human ones. Like a seed crystal dropped into a supersaturated solution, “cognitive assemblages” might do the trick.	Stewart's work recognizes the consequences of computers reading for us before returning to familiar territories of conceptual art and the 'work of words'. However, it lacks the perception that computers also possess cognition, producing interpretations and meanings within both machine and human contexts. The concept of 'cognitive assemblages' could be the missing piece.	Integrating Cognitive Assemblages in Book Studies	 "Cognitive assemblages" recognizes computers as cognition-capable entities, creating and sharing meanings in both machine and human contexts.
Another way in which this study aligns with book studies while nevertheless bringing a new configuration into view is in the emphasis on the everyday. Through interviews, archival research, and textual analyses, Amy Hungerford’s Making Literature Now conveys a vivid impression of what it means to work as a writer and creator of literature in the contemporary moment. Especially useful are her distinction between celebrity and subsistence writers (that is, writers who cannot make a living writing) and her insightful analysis of how each kind of writer depends on the other. The celebrity writer not only solicits work from subsistence writers but also uses their (usually underpaid and sometimes unpaid) labor to create an aura of “art for art’s sake” that enhances the cultural capital of the celebrity writer’s own (usually highly paid) work. Of course, it is not news that celebrity writers hand off certain writing tasks to subsistence writers (think ghostwriters), but Hungerford shows that the present configurations of digital technologies with print books enable new kinds of circular payoffs for both.	Amy Hungerford's Making Literature Now sheds light on the realities of contemporary literature creation through interviews, archival research, and textual analysis. She distinguishes between celebrity and subsistence writers, analyzing how they depend on each other. Celebrity writers often exploit the labor of subsistence writers to enhance their own cultural capital, and Hungerford shows that current combinations of digital technologies with print books allow novel benefits for both parties.	Contemporary Literature Creation and Its Complexities	The interplay of digital technologies and print books reshapes the relationship between celebrity and subsistence writers in the contemporary literary scene.
One of Hungerford’s case studies is the digital version of The Silent History (discussed here in chapter 4 as the print codex published when the app version was completed), with Eli Horowitz as master creator enlisting app users to submit “Field Reports” that extend the narrative via Global Positioning System (GPS) technology into local habitats and local knowledge. Although she does not mention it, we might also think of Ship of Theseus, the collaborative project between celebrity media creator J. J. Abrams, who masterminded the concept, and Doug Dorst, whom Abrams enlisted to write the narratives. It is no accident that this codex, like The Silent History, is formally innovative, for both works are postprint productions completely interpenetrated by the digital technologies crucial for their physical instantiations.	Hungerford's study includes the digital version of The Silent History, where Eli Horowitz incorporated user-submitted "Field Reports" using GPS technology. This extends the narrative into local environments. Another example is the collaboration between J.J. Abrams and Doug Dorst on Ship of Theseus. Both works are innovative postprint productions, demonstrating the significant influence of digital technologies on their creation.	Digital Innovations in Contemporary Literature	Digital technologies, encompassing user contributions and collaborations, are fundamentally transforming narrative creation in today's literature.
Although Hungerford emphasizes the personal connections between writers and publishers (especially McSweeney’s), she is also alert to differences between digital instantiations and print books. Her chapter on The Silent History recognizes that the digital instantiation is actually quite different from the print version (see chapter 4 for an analysis of how the codex version’s verbal text differs from the app version’s). Moreover, she is quick to make the connection between users of the app and the silent protagonists of The Silent History, who are alienated from normal social relations by their inability to learn or understand verbal language. In both the app and codex versions, we learn that the “silents,” as they are called, are subjected to mandatory computational implants that enable them to speak, but at the price of confining their articulations solely to the words and phrases in the computer’s databases. In direct address to the reader, Hungerford makes the connection between the implanted silents and the app users (who may be hearing an audio version of the narrative through headphones): “You find yourself manipulated by a central processor that pipes language into your brain … [and that] makes us into the very figure of the wandering silent.” The recognition that the “central processor” is a cognitive actor trembles on the brink of visibility here. If brought into full view, it would enable a cascading series of realizations about what it means for humans to act within computational environments that are capable of performing interpretations and creating meaningful contexts, some of which are aimed at machine addressees rather than human ones. Here the new configuration, once perceived, would enlarge the scope of inquiry so that it pertains not just to a single digital work but to our contemporary condition as actors within cognitive assemblages.	Hungerford discerns the differences between digital and print versions of "The Silent History" and compares the user's interaction with the app to the silent protagonists who receive mandatory computational implants. This recognition opens a discussion about humans functioning within computational environments capable of interpretation and creation of meaningful contexts.	Bridging Human and Machine Interactions	Hungerford's analysis suggests computation as an active participant in human-computer interactions within cognitive assemblages.
In the field of book history, Ted Striphas perhaps comes closest to the interests of the present study in The Late Age of Print when he interrogates some of the technologies that made the contemporary codex into a mass-produced commodity integrated with digital technologies in its production, distribution, marketing, and consumption. It is interesting that in 2009 he still found it useful to pick up on Jay David Bolter’s phrase “the late age of print” for his title, referring to Bolter’s study published in 1990 in a preweb era that is now apt to seem quaintly old-fashioned. Striphas quotes Bolter’s assertion that “just as late capitalism is still vigorous capitalism, so books and other printed materials in the late age of print are still common and enjoy considerable prestige.”30 Of course, printed books are still common, but whereas the phrase late age emphasizes continuities, the term postprint acknowledges both continuities and ruptures. Moreover, postprint denotes that far more than the physical artifact is involved, extending the transformation’s scope into every aspect of book production and consumption. In addition, the cognitive-assemblage framework developed here links these artifactual and technological changes with large-scale cultural, economic, and psychological processes through the operations of hybrid human–technical collectivities crucial to contemporary developed societies. Striphas writes that his focus on the everyday has the advantage of concentrating on change “without presuming a full-blown crisis exists.” I agree with his strategy of avoiding an apocalyptic tone, although I would add that precisely because cognitive assemblages have become the everyday does not mean that they are not effecting transformations of massive scope and import. They are both banal and momentous, everyday and utterly transformative.	Striphas's study in "The Late Age of Print" analyses the technologies involved in the production and consumption of contemporary codex, stressing on the everyday changes rather than a looming crisis. However, the term "postprint" and the concept of "cognitive assemblages" encapsulate a broader transformation, linking these changes with larger cultural, economic, and psychological processes, acknowledging their significant, transformative impact.	Cognitive Assemblages in the Late Age of Print	The integration of digital technologies in book production and consumption is both commonplace and transformative, driving significant cultural, economic, and psychological shifts.
For my purposes, the clearest example of how digital technologies affect contemporary book production comes in Striphas’s chapter on the history and implementation of the International Standard Book Number (ISBN). He cites a report from O. H. Cheney in 1932 asserting that the major chokepoint in the book industry’s expansion was its lack of standardized distribution mechanisms, thus highlighting the importance of back-office operations for the entire trade. The solution, Striphas explains, included the development of standardized numbers that would uniquely characterize each volume. He makes the point that the “ISBN isn’t merely a glorified stock number. Rather, it’s a carefully conceived, highly significant, and mathematically exact code that contains detailed information about the identity of each book,” including information about the book’s language, region or nation, publisher, title, edition, and binding. “Corporate booksellers,” he continues, “make books fungible, commensurable with one another. The ISBN was a crucial back office counterpart of these processes.”	Striphas's chapter highlights the significance of the ISBN in book industry, as it provided a standardized mechanism that improved distribution by uniquely identifying each book's key details and thereby facilitated the expansion of the book industry.	ISBN and its Role in the Book Industry	The implementation of the ISBN system significantly streamlined the book industry's distribution mechanisms.
So useful had ISBN codes become by 1980 that the International ISBN Agency contacted its counterpart, the European Article Number (EAN) International, to integrate the two systems in ways that would account for currency and national differences, which required that a prefix referring to the country of origin be added to the ISBN. Charmingly, the solution was to invent the fictious country of Bookland, denoted by the prefix 978. It was not until several years later, in the late 1980s, that the U.S. book industry finally agreed that all books sold in bookstores “would be imprinted exclusively with the Bookland EAN bar code.” It is no coincidence that this period was also the era when mainframes were being replaced by desktop computers, making computational functionalities accessible to every bookstore and book distributor. Although Striphas does not emphasize the point, the incorporation of ISBNs into computer databases via digital optical readers in the 1980s greatly expanded the ISBNs’ utility, allowing them to be integrated with other information about books in ways that vastly improved not only distribution but also warehousing, inventory, shelving, and sales records. Striphas’s account vividly demonstrates how important back-office operations are to the organization of the book industry. It follows that the penetration of computational processes into these operations also had enormous consequences for everyday practices that affected not only how books became mass-market commodities but also how they were—and are—produced, edited, formatted, warehoused, sold, and read: in brief, how books began to function within the cognitive assemblages crucial to contemporary developed societies.	 The integration of the ISBN with the European Article Number (EAN) system and its digitization in the 1980s transformed the book industry by improving various aspects including distribution, warehousing, inventory management, shelving, and sales records, making books function within the crucial cognitive assemblages of contemporary developed societies.	Digitization of ISBN and Transformation of Book Industry	The digitization of the ISBN system significantly transformed the book industry, integrating it into the cognitive assemblages of modern societies.
Positioned somewhere between book studies and media studies is Alexander Starre’s analysis of what he calls metamedia. He uses the term to demonstrate that the penetration of computational media into composing practices has affected the structures and meanings of literary texts. “Broadly speaking,” he writes, “a literary work becomes a metamedium once it uses specific devices to reflexively engage with the specific material medium to which it is affixed or in which it is displayed.” As he acknowledges, in Writing Machines I developed a similar concept of “technotext,” a literary work that reflects within itself the technological conditions of its composition and production. I have also discussed some of the texts he analyzes in terms of their self-reference to digital technologies, including Mark Danielewski’s House of Leaves and Jonathan Safran Foer’s Extremely Loud and Incredibly Close and Tree of Codes. Starre is careful to delineate the ways in which his work diverges from mine, but the broader point, I think, is that we agree that digital technologies have deeply influenced not only how books are produced but also how they are conceived and written. “My argument about medial self-reference takes on its full significance when sketched out against the background of a different medium,” he writes—that is, when seen in the context of digital technologies as distinct from paperbound books.	Alexander Starre's concept of "metamedia" reflects how computational media has influenced the structure and meaning of literary texts, signifying their self-awareness of the technological conditions of their composition and production.	Metamedia: Literature's Self-Awareness in the Digital Age	Digital technologies have profoundly influenced the conception, writing, and self-referential nature of literary texts.
If I had to critique Starre’s approach, I would focus on his claim that the term metamedia (which I like a lot) is broader than the term technotext because it corrects what he calls the “technological bias” of the latter. Whether my approach consists of a “bias” toward technology or a focus on it depends on how important one considers technology, in particular computational media, in our contemporary moment. My argument here is that computational media, in addition to transforming everyday practices in innumerable ways, have also fundamentally altered the nature of cognition, repositioning human actions so that they increasingly operate within the contexts of cognitive assemblages. Of course, this repositioning affects how print books are conceived and written in the digital era, but the full scope of the transformation is much broader. Although the concepts of postprint and becoming computational include the phenomena that Starre references through metamedia, they also bring into view an emerging configuration in dynamic interplay with older formations associated with print culture, as we will witness, for example, in chapter 3, on how digital technologies are transforming scholarly publishing.	The transformative role of computational media extends beyond everyday practices, fundamentally altering cognition and impacting the conception and writing of print books in the digital era.	Broad Reach of Computational Media	Computational media significantly changes cognition.
Media Archeology, Cultural Techniques, and Cognitive Assemblages			
			
In addition to overlapping with history of the book and book studies, the cognitive-assemblage approach has commonalities with media archeology as well as some differences. According to Wolfgang Ernst, media archeology (in Ernst’s spelling, “archaeology”) proposes a countermethodology to a human-centered narrative approach, considering events from a nonhuman point of view, in particular the viewpoint of technical media. As Ernst puts it, “The media-archaeological method … is meant as an epistemologically alternative approach to the supremacy of media-historical narratives.”38 Focusing on technical media, Ernst characterizes them by pointing to the difference between painting and photography. Whereas the materials required by painting are always accessible to human senses, the active processes in photography (leaving aside setting the speed, aperture, and focus and clicking the shutter) are physiochemical reactions and are not generally directly accessible by human users. Today’s camera, considered distinct from the human operator, has a cognitive viewpoint, and one of the goals of media archeology is to explore this viewpoint as fully as possible. Working from a different set of premises, Ian Bogost accomplishes a similar task in his discussion of the Foveon-equipped Sigma DP digital-image sensor.39 As his analysis makes clear, media have bodies; that is, they are materially instantiated. And their bodies are increasingly equipped with sensors, actuators, and information processors. They perform interpretations and arrive at meanings specific to the contexts in which they operate and the functions they are designed to perform.	Media archeology provides an alternative, non-human-centric viewpoint, focusing on technical media and their inherent functions and interpretations. Similarly, as materials evolve, devices have physical bodies with sensors, actuators, and information processors, which shape their interpretations and meanings.	Beyond Humans in Media Archeology	 Technical media have their own non-human perspective and functions, leading to unique interpretations and meanings.
In practice, media archeology often focuses on legacy media such as early cinematic devices, telephones, telegraphs, and so forth. The tendency in media archeology is to promote, as Ernst describes it, an “awareness of moments when media themselves, not exclusively humans anymore, become active ‘archaeologists’ of knowledge.” The differences between narrative and media archeology deepen when the devices are cognitive or, as Ernst writes, when “techno-epistemological configurations” underlie the “discursive surfaces”—that is, when software architectures generate the output that humans see. If one focuses only on the (human-centered) display, the machine’s viewpoint is lost, and one consequently has little or no idea of what is actually generating the display and the results it conveys.	Media archeology often focuses on older media, raising awareness of moments when media themselves become 'archaeologists' of knowledge. When dealing with cognitive devices, focusing solely on human-centered display could obscure the understanding of underlying software architectures generating the output.	Media Archeology and Legacy Media	Older media and cognitive devices offer unique perspectives that may get lost if we only focus on the human-centered display.
Ernst’s assertion that devices are “ ‘archaeologists’ of knowledge” claims more than his argument rightly earns, but in the present environment of networked and programmable machines it can be justified. For example, emulators (or virtual machines) are designed to mediate between an older operating system and a newer one, translating between legacy and newer code so that files can be read and commands can be executed that otherwise would be unreadable and inoperable. Such a machine can be considered an “archeologist” (whether it is an archeologist of knowledge would require a longer discussion about what “knowledge” means in a machine context). The media-archeology approach positions human subjectivity, especially its inclination toward narrative, as a limitation from which a machine viewpoint can liberate us. Such a perspective can wean us away, as Ernst says, from “wanting to tell stories,” which only serves to “reinforce our subjectivity,” and instead catalyze awareness of how machines actually work—that is, how they operate through nondiscursive methods.	Machines like emulators can serve as 'archaeologists' by translating between old and new systems, thus enabling understanding of outdated codes. Media archaeology encourages a shift from human-centric narratives to understanding the non-discursive functioning of machines.	Emulators as Knowledge Archaeologists	Machines, like emulators, can provide deeper understanding by translating between old and new systems.
The media-archeology approach has much to recommend it. It opens up explorations of machine capabilities in their own right and activates Foucauldian questions about such crucial matters as the nature of the archive beyond language, in which, as Jussi Parikka puts it, “power is no longer circulated and reproduced solely through … practices of language, but takes place in the switches and relays, software and hardware, protocols and circuits of which our technical media systems are made.” A limitation, apparent in Ernst but less true of media archeology in general, is a tendency to create an either/or binary between human and machine viewpoints. In the contemporary era, combinations of machine and human perspectives are far more typical, as theorists such as Parikka and Lori Emerson recognize and exploit in their analyses.	Media archaeology presents insights into machine capabilities, prompting questions about power beyond language. However, it tends to oversimplify the human-machine relationship, while the reality is more intertwined.	Media Archaeology's Strengths and Limitations	Media archaeology provides valuable insights into machine capabilities but often oversimplifies the complex relationship between humans and machines.
The latter point has another aspect, too: in focusing only on the technical capabilities of media, salutary as that may be, the media-archeology approach sometimes leaves to the side the historical circumstances in which those devices were created, including why the inventors focused on certain functionalities rather than on others, why some devices were commercially successful and others were not, and how the devices transformed the media ecologies in which they were embedded. As Simondon writes in the introduction to On the Mode of Existence of Technical Objects, “In technical reality there is a human reality,” though he also insists on the converse, that in human reality there is technical reality. He continues: “If [a focus on technical objects] is fully to play its role, culture must come to terms with technical entities as part of its body of knowledge and values.” Writing in the 1950s and 1960s, Simondon was ahead of the curve. The narrative approach that arguably dominated media history and theory through the 1980s received a shock to thought when in 1992 Friedrich A. Kittler’s Discourse Networks 1800/1900 burst upon the scene: thereafter, media theory took a decisive turn away from narrative and toward the materialities of media.	Media archaeology's focus on technical media capacities often overlooks the historical contexts and human aspects of technological development.	The Human Factor in Media Archaeology	 In studying media archaeology, it's essential to consider the historical and human factors that shape technological development and use.
In this sense, the media-archeology framework (along with the Kittlerian thrust of what is somewhat problematically called “German media theory”) provides a useful corrective. As Ernst makes explicit, however, it does so at the expense of exploring the human reality entwined with the technical: “Media archaeology concentrates on the nondiscursive elements in dealing with the past: not on speakers but rather on the agency of the machine.” Because media archelogy tends to obscure the human reality entwined with technical reality, it falls short of being a complete account of the cognitive operations of technical devices. Chapter 2 adopts a media-archelogy approach to the extent that it emphasizes the cognitive capabilities of the typesetting machines that evolved from the late nineteenth century to the end of the twentieth century. Throughout, however, it also attends to the humans who created the machines, discussing their motives, organizational contexts, and distinctive contributions. It thus benefits from media-archeology perspectives (a debt I am pleased to acknowledge) but combines them with historical-narrative approaches to present a fuller picture of how the human side of cognitive assemblages interact with the technical cognizers emerging within computational media.	 Media archaeology studies technology from a non-human perspective, but often overlooks the human-technical intertwined reality. While highlighting machine functions, it sometimes misses their historical context and human elements. This chapter combines these viewpoints for a balanced understanding of technology's evolution.	Balance in Technology Analysis	Understanding technological evolution requires considering both machine capabilities and the human elements involved in their creation and evolution.
Perspectives focusing on “cultural techniques” have recently emerged as contenders for the title “German media theory.” Bernhard Siegert’s excellent introduction to his book Cultural Techniques articulates the central ideas of this approach and locates it within media theory more generally. Siegert quotes Thomas Macho’s argument that “cultural techniques—such as writing, reading, painting, counting, making music—are always older than the concepts that are generated from them. People wrote long before they conceptualized writing or alphabets; millennia passed before pictures and statues gave rise to the concept of the image; and to this day, people may sing or make music without knowing anything about notes or musical notation systems.” The argument, then, is that culture “bubbles up” from the everyday use of objects and only retrospectively solidifies into concepts, rituals, and abstractions that, according to some, constitute what can be considered “cultural” beliefs or even “culture” as such. Expanding on this assumption, Siegert writes that “operations such as counting or writing always presuppose technical objects capable of performing—and to a considerable extent, determining—these operations.” By focusing on technical objects (construed quite broadly as including doors, filters, and grids, each the subject of one of Siegert’s chapters), the cultural-techniques approach claims to have a view of culture at its source, uncontaminated by the biases and misperceptions lurking in accounts that start from the assumption that culture is what people say or think rather than what they do.	Cultural techniques approach argues that everyday use of objects forms culture before it solidifies into concepts. Technical objects, capable of performing operations like counting or writing, are central to these operations. This approach perceives culture as what people do rather than what they say or think.	Cultural Techniques: Action Over Abstraction	The cultural techniques perspective asserts that culture emerges from the practical use of objects and daily actions rather than abstract concepts and beliefs.
Although sharing with media archeology a focus on objects, the cultural-techniques approach diverges from it in taking a keen interest in how objects are created, used, modified, and discarded, which necessarily expands the scope of the approach to human interactions with technical objects. Humans, in this view, are constituted through the objects they fabricate and the practices through which objects are put to use: “Humans as such do not exist independently of cultural techniques of hominization,” Siegert asserts. The interest for Siegert is not so much on objects by themselves or humans by themselves but rather on the interactions that connect them. With a nod to Latour’s actor-network theory, he writes, “When we speak of cultural techniques, therefore, we envisage a more or less complex actor-network that comprises technological objects as well as the operative chains they are part of and that configure or constitute them.”	The cultural-techniques approach stresses the importance of the creation, use, modification, and disposal of objects and their interaction with humans. This approach views humans as entities shaped through their interaction with the technical objects they produce and utilize.	Interplay of Humans and Objects	Cultural techniques view humans and objects as intertwined, each shaping the other through their interactions.
The cognitive-assemblage approach travels along the same path as media archeology in affirming that objects may have viewpoints, and it shares with the cultural-techniques approach an intense interest in understanding the chains of connection and causality that enmesh humans and objects together into assemblages. The main point of difference lies in the emphasis on cognitions as processes central to understanding how cognitive assemblages are constituted, function, adapt and evolve, and have become pervasive in contemporary societies. The design of simple everyday objects such as the doors, grids, and filters that Siegert discusses now increasingly involve computational media (through computer-aided design and computer-aided manufacturing and other kinds of simulation software); in the case of automated factories, their fabrication also runs through computational devices (such as industrial robots). Hence, in the cognitive framework the emphasis falls not solely or even primarily on hominization as creating distinctions between humans and nonhuman animals (Siegert’s “humans as such”) but rather on the ways in which humans are now inextricably bound up with networks of intelligent devices, from jet aircraft to underground fiber-optic cables and almost everything else in between.	Cognitive-assemblage approach, unlike media archaeology and cultural techniques, centers around cognition. It acknowledges the influence of computational devices on both design and manufacturing processes in today's societies.	Cognitive Assemblages and Computational Influence	The cognitive-assemblage approach underlines how cognitive processes and intelligent devices shape the creation of everyday objects.
The subtitle of Siegert’s book ends with Articulations of the Real, indicating that cultural techniques not only interact with and help to define the human but also interact with the world as it exists independent of human perceptions. “The analysis of cultural techniques observes and describes techniques involved in operationalizing distinctions in the real. They generate the forms in the shape of perceptible unities of distinctions. Operating a door by closing and opening it allows us to perform, observe, encode, address, and ultimately wire the difference between inside and outside.” Similarly, the cognitive-assemblage framework acknowledges that cognitive devices do more than interface with humans; they also interact with one another and through more or less complex networks of sensors, actuators, and transmitters perform actions independent of human interventions and in many instances removed from human knowledge of their actions. The emphasis on cognition implies that intelligent devices not only “articulate the real” but also, as my definition of cognition emphasizes, interpret it as well. Diverging from both media archeology and cultural techniques in this respect, the cognitive-assemblage framework raises large questions that emerge when meaning-making practices, not exclusively human anymore (if they ever were!), also extend to nonhuman life-forms and computational media.	Cultural techniques interact with and shape reality beyond human perception, contributing to forming the real world. The cognitive-assemblage approach further emphasizes that intelligent devices also interpret reality. This involves acknowledging that meaning-making is not solely a human activity, but extends to nonhuman life and computational devices, raising new questions in the process.	The Intersection of Cultural Techniques and Cognitive Assemblages	Intelligent devices not only articulate reality, they interpret it, thus challenging the idea that meaning-making is solely a human activity and opening up new areas of inquiry.
Chapter 3, featuring interviews with personnel at five university presses, illustrates the specific questions that arise when the conscious assumptions and nonconscious intuitions of humans practicing cultural techniques interact with computational media. Directors, editors, and other staff involved in university book production bring to their tasks deeply held beliefs about what books are as well as about what scholarship is and should be. Their beliefs are rooted in years of experiencing books as intellectual projects scrutinized, interrogated, and evaluated for their intellectual contributions and as physical artifacts designed, handled, and appreciated for their aesthetic qualities. As computational media take over tasks formerly done solely by humans, such as designing from templates, these people face everyday consequential decisions about how much they want to rely on technical cognition and how much they want to preserve as enclaves in which human creativity and decisions are paramount. Nowhere is this clearer than with university presses that are developing online-publishing venues, such as the University of Minnesota’s Manifold platform. The necessity to prepare a text so that it conforms to what computational media can process is in constant active tension with the desire to make the product express what its human creator envisioned. Of course, there are opportunities as well as constraints in this dance between human and technical cognition. The complexities of the situation require a new kind of approach that draws on media archeology and cultural techniques but goes beyond both in its attention to the kinds of interactions emerging from and constitutive of the cognitive assemblages so crucial in today’s developed societies.	University press personnel face decisions about the role of computational media in tasks like book design, often balancing the constraints and opportunities presented by technology. The transition to online-publishing platforms involves a tension between maintaining the author's vision and adhering to what computational systems can handle. Addressing these complexities necessitates an approach combining elements of media archeology and cultural techniques, while also focusing on the unique interactions forming cognitive assemblages crucial in modern societies.	Balancing Human Creativity and Computational Media in Book Publishing	The contemporary publishing landscape necessitates a balance between human creativity and computational capabilities, a dynamic best understood through a hybrid approach blending media archeology, cultural techniques, and study of cognitive assemblages.
 What Lies Ahead 			
Books have long been recognized as devices that support and extend human cognition, from their earliest instantiation in the vertically inscribed reeds of Chinese jiance to classical scrolls, from medieval manuscripts to mass-produced modern print books. Only in the late twentieth century, however, did books begin to function as cognizers, capable of knowing where they are, how long their user has been reading a particular passage, who else has read and underlined that passage, along with any number of other interpretations and meaning-making practices. Coincident with the development of such e-readers were other machines with cognitive capabilities that fundamentally changed the nature of printing, completely transformed the book industry in its everyday practices, and critically altered the distribution of cognitive tasks among humans and machines. It would take many volumes to explore the full significance of these changes, and exploring the correlative transformations in how we understand the human would take exponentially more. Many studies investigating these questions have already appeared, from Manuel Castells’s classic trilogy on the Network Society to, more recently, Mark Hansen’s Feed-Forward: On the Future of Twenty-First-Century Media; Erich Hörl’s General Ecology: The New Ecological Paradigm; and many others, including my own work from 1999 on. The inquiries of this volume are more specifically focused on the typesetting and reprographic machines that became cognitive, the publishers who work with these machines, and the effects on human self-conceptions when code intervenes between textual displays and underlying algorithmic processes.	Throughout history, books have served as tools for human cognition, but the late 20th century saw them evolving into cognizers themselves, interacting with users in new, dynamic ways. This transformation, along with other machine-driven changes in the printing industry, has significantly altered the distribution of cognitive tasks between humans and machines, prompting profound shifts in how we understand human nature.	Cognitive Evolution in Books and its Implications	The development of cognizing books and advanced printing technologies has redistributed cognitive tasks between humans and machines, leading to a reevaluation of our understanding of human nature.
Chapter 2, “Print Into Postprint,” explores five points at which printing technologies began to acquire cognitive capabilities. The theme of embodiment enters this chapter through the description of how the machines were designed and constructed (that is, how they were instantiated), beginning with the Paige Compositor. It is scarcely a new observation that the easiest way to imagine technological change is to start with what already exists and introduce a few modifications. Only gradually do design imperatives begin to assert themselves from what the new object is in itself, independent of the devices from which it descended. Early bathtubs looked a lot like the barrels from which they started, early cars resembled buggies, and so forth. The Paige Compositor was the first typesetting machine that might be said to have cognitive capabilities, and its antecedent was the human compositor. Inventor James Paige’s idea was to duplicate in mechanical form all the motions that a human compositor used to compose a line of type. In this decision, he doomed his machine to the worst of two worlds: the machine lacked the robustness of mechanical devices as well as the self-repairing nature of human embodiment and cognition that would allow it to recover from error and breakdown. As a result, the Paige Compositor was a commercial failure, although one could argue it was a conceptual triumph.	Chapter 2 delves into how printing technologies began to acquire cognitive abilities, starting with the Paige Compositor. Although a commercial failure due to its complexity, the Paige Compositor was a pioneer in blending human and mechanical actions.	Inception of Cognitive Abilities in Printing	The journey towards cognitive capabilities in printing technologies started with devices like the Paige Compositor that attempted to emulate human actions.
Another nodal point is the invention of the optical typesetter, specifically the Lumitype. The histories of science and technology are replete with instances in which an outsider to a field is able to make a dramatic breakthrough precisely because he is positioned at the margins. The physicist Erwin Schrödinger influenced generations of molecular biologists with his little book What Is Life?; Harold Hidebrand, a petroleum engineer, invented AutoTune, a device that corrects a singer’s voice in a recording so it always hits the right pitch, when he applied the digital-signal processing he used for oil exploration to the human voice; and so forth. In the case of the Lumitype, the outsider influence came when an electrical engineer, Louis Moyraud, teamed up with René Higonnet, an amateur photographer, neither of whom had previous experience with printing. Once again, embodiment is crucial because the machine they invented fundamentally changed the typesetting process from metal type pieces inking paper to flashes of light exposing a photographic plate.	The Lumitype, a revolutionary optical typesetter, was created by outsiders to the printing field. This machine transformed the typesetting process from a mechanical to a photographic procedure.	Lumitype: Outsider Innovation in Typesetting	Significant innovations often come from outsiders, such as the Lumitype's shift from mechanical to photographic typesetting by inventors not originally involved in the printing industry.
If we fast-forward to the 1990s, the invention of Xerox’s DocuTech is perhaps most notable for its engagement with corporate politics and the light it casts on what it takes for new ideas to take hold in a corporate environment. It reveals the gap between the individual inventor working on his own in a basement (as Higonnet and Moyraud were) to teams of engineers headed by a project manager, whose job it is not only to bring the new product to market but also to navigate all the perils of corporate competition with rival teams and jealousies among those pursuing alternative solutions.	 Xerox's DocuTech's development highlights how corporate innovation requires navigating technical challenges and internal politics.	Corporate Politics and Innovation: The Case of Xerox’s DocuTech	Innovation within a corporate setting, as exemplified by Xerox's DocuTech, involves not only technological development but also the skillful navigation of internal politics and rivalries.
Whereas chapter 2 focuses on cognitive machines and the people who invented them, chapter 3, “The Mixed Ecologies of University Presses,” changes the perspective to editors, publishers, designers, and scholars who do not invent the technologies but find them indispensable for their work after others have brought the machines to market. Thus, chapters 2 and 3 address the two kinds of entities involved in cognitive assemblages, respectively: machines with cognitive abilities and humans who utilize those machines. In chapter 3, the inertia of centuries of print traditions comes front and center, operating both within university presses and in the ways scholarly productivity is conceptualized and evaluated within the academy. Given the discussion at the end of chapter 2 of the difficulties of incorporating ethical considerations into technical innovations within capitalistic contexts, this inertia in humanities scholarship is not unequivocally a bad thing, although there are forceful arguments that university presses and tenure committees urgently need to update their worldviews more fully to take into account how scholarship is changing in the digital era. Nevertheless, even in these relatively traditional environments, changing practices within some presses and corresponding reforms within some tenure and promotion committees are demonstrating a movement toward postprint, resulting in the present situation of mixed ecologies. Among this chapter’s noteworthy insights are the changing shapes of scholarly careers as researchers move toward websites rather than to print books and as younger scholars transition toward more public-facing discourses that the web enables.	Chapter 3 shifts focus to those who utilize, rather than invent, cognitive technologies in their work – namely, editors, publishers, designers, and scholars. The chapter illuminates the challenges of reconciling centuries-old print traditions with the demands of the digital era, leading to "mixed ecologies" in scholarly publication. It also highlights the evolving landscape of academia as scholars increasingly move towards more public-facing and digital platforms.	The Shift from Print to Digital in Academic Publishing	The adoption of cognitive technologies is causing significant change in academic publishing and altering traditional career paths as scholars increasingly utilize digital platforms.
Chapter 4, “Postprint and Cognitive Contagion,” analyzes two fiction texts, The Silent History and The Word Exchange, to explore the hopes and fears aroused by leaving the regime of print and entering into postprint. Because contemporary human cultures are heavily invested in writing and the written word, the sea changes initiated as signs became digital and thus deeply entwined with computational media are transforming the functionality and meaning of human cognition. As Dennis Tenen argues and as these texts explore, postprint insinuated a new condition of partial illiteracy into the heart of literate cultures. The Silent History, a text that bears the mark of the digital in its genesis and realization, depicts this new condition as both loss and gain. In contrast, The Word Exchange presents a darker vision of human cognition enfeebled and endangered by the advent of postprint, urging a return to traditional print as the only effective remedy.	Chapter 4 examines two fiction texts, The Silent History and The Word Exchange, to understand the implications of the transition from print to postprint culture. This shift has brought about a new condition of partial illiteracy and is transforming human cognition. While The Silent History sees this change as a mixture of loss and gain, The Word Exchange presents a grimmer picture and suggests a return to traditional print.	Fiction's Perspective on Print to Postprint Transition	Transition from print to postprint culture is transforming human cognition and introducing a new kind of partial illiteracy, as illustrated in The Silent History and The Word Exchange.
Once the genie is out of the bottle (that is, once code generates the visible sign), however, language cannot so easily return to its former status as a production of human cognition alone. Chapter 5, “Bookishness at the Limits: Resiting the Human,” follows this line of thought to its limits in two different directions: to the complete immersion of the human reader in a cognitive assemblage, on the one hand, and to books that could never be read by computers because they have no legible words, on the other. The former is explored through Between Page and Screen by Borsuk and Bouse, mentioned earlier as an example of a book that can be read only with a computer. Books that can be read only by humans are interrogated through the work of Mirtha Dermisache, an Argentinian artist specializing in creating asemic productions—that is, human mark making that escapes alphabetization. In the contemporary moment, asemic inscriptions are celebrated as resistant practices to algorithmic instrumentality, and so they also bear the mark of the digital but as a movement against rather than a motion forward into algorithmically enabled reading and writing. But this resistance also has an intrinsic ambivalence about it. Because the marks in Dermisache’s books cannot be read in a conventional sense, her mark making can be seen as pointing toward the kind of conclusion Vilém Flusser advocates when he argues that the advent of code spells the end of alphabetic writing; from his point of view, alphabetic writing has no future.54 Nevertheless, the fact that Dermisache is adamant that her work should appear as bound codices—that is, as books—suggests that the book as a cognitive and artistic form is endlessly resilient and will continue even if its contents no longer represent verbal language.	Chapter 5 looks at two extremes in bookishness - a book that can only be read with a computer, and those that are human-readable only, with no legible words. It highlights Between Page and Screen, read only with a computer, and the work of artist Mirtha Dermisache, creating asemic (non-alphabetic) inscriptions, a form of resistance to algorithmic control. Despite the advent of code-based writing, Dermisache's insistence on bound codices suggests the enduring resilience of the book form.	From Machine-read to Asemic Books: Two Extremes	The changing realm of 'bookishness' is explored through two extremes: computer-dependent reading and human-only, asemic writing, illustrating both the transformative impact of code-based writing and the enduring resilience of the physical book.
This book aspires to contribute to materialist studies of print, to media studies focusing on the interactions of print and computational media, and to cognitive-cultural studies of the influence of computational media on concepts of the human. Perhaps its greatest ambition is to present the cognitive-assemblage framework as a way to integrate all these areas into a coherent methodology and theoretical perspective. Whether it achieves some or all of these goals is, of course, not up to me but to the book’s readers. Whatever its fate as an intellectual project, its very existence as a text composed, edited, designed, produced, marketed, and sold (and in some instances read) by computational media witnesses the advent of postprint. Print is dead; long live postprint!	This book aims to blend studies of print, computational media, and cognition into a comprehensive methodology, introducing the cognitive-assemblage framework. Its very creation and existence, aided by computational media, mark the era of postprint, embodying the transformation from traditional print.	A New Era: Cognitive Assemblage and Postprint	The book seeks to merge various disciplines into a cohesive theory with the cognitive-assemblage framework, its creation and existence marking the transition to the era of postprint.